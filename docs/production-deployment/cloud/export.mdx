---
id: export
title: Workflow History Export
sidebar_label: Workflow History Export
description: Workflow History Export in Temporal Cloud lets users export Closed Workflow Histories to an object storage for compliance and analytics. Configure via Cloud UI or tcld.
slug: /cloud/export
toc_max_heading_level: 4
keywords:
  - archival
  - export
  - operations
  - temporal cloud
  - verify
  - workflow history export
tags:
  - export
  - Temporal Cloud
  - workflow history
---

:::tip Support, stability, and dependency info

Workflow History Export is in [Public Preview](/evaluate/development-production-features/release-stages#public-preview).

:::

Workflow History Export allows users to export closed Workflow Histories from Temporal Cloud to cloud object storage (AWS S3 or GCP GCS), enabling:

- Compliance and audit trails of complete Workflow History data in [proto format](https://github.com/temporalio/api/blob/master/temporal/api/export/v1/message.proto)
- Analytics on Workflow History when ingested to the data platform of your choice

Workflow History Export in Temporal Cloud provides similar functionality as [Archival](/self-hosted-guide/archival) in a self-hosted Temporal Server. Archival is not supported in Temporal Cloud.

Exports run hourly, beginning 10 minutes after the hour. The time at which a closed Workflow appears in the exported file may vary, but it is typically available within one to four hours. Delivery is guaranteed at least once.

At least once delivery is guaranteed.

## Prerequisites {#prerequisites}

To use Workflow History Export, you must have:

1. A cloud account in the cloud provider where your namespace is hosted.
2. An object storage bucket available to receive the exported History.

## Configure Workflow History Export {#configure}

### AWS

[AWS S3 Export Configuration](/cloud/export/aws-export-s3)

### GCP

[GCP GCS Export Configuration](/cloud/export/gcp-export-gcs)

## Verify export setup {#verify}

From the Export configuration page, select **Verify**.
This action checks if Temporal can successfully write a test file to your object storage.

If everything is configured correctly, you will see a `Success` status indicating Temporal has written to the object store.

## Monitor export progress {#monitor}

After Export has been configured, you can check that it's still working in several ways:

1. **Object Storage**:

   - File Delivery: After the initial hour of setting up, inspect your object storage.
     You should see the exported Workflow History files.
   - Directory Structure: Your exported files will adhere to the following naming convention and path:

   ```bash
   //[bucket-name]/temporal-workflow-history/export/[Namespace]/[Year]/[Month]/[Day]/[Hour]/[Minute]/
   ```   

2. **Temporal Cloud Web UI**:

   - Export UI:

      - Last Successful Export: This displays the timestamp of the most recent successful export.
      - Last Status Check: This reflects the timestamp of the latest internal Workflow healthcheck.

   - Usage Dashboard:
      - Actions from the Export Job are included in the [Usage Dashboard](/cloud/billing-and-cost).

3. **Metrics**:
   - Export-related metrics are available from the [Cloud metrics endpoint](/cloud/metrics/), specifically the metric `temporal_cloud_v0_total_action_count` with the label `is_background="true"`.

## Working with exported files

Use the proto schema defined in https://github.com/temporalio/api/blob/master/temporal/api/export/v1/message.proto to deserialize exported files.

## Export and High Availability Namespaces {#export-ha}

### Export Region Persistence

When Export is configured for a [High Availability](/cloud/high-availability) Namespace, the export is tied to the specific region where it was initially set up. The export configuration does not automatically failover with the namespace.

- If Export is configured in Region A, it will continue to export from Region A's storage even after a namespace failover to Region B
- Exports always read from and write to the same region where they were originally configured
- The export process is independent of namespace failover events
- Export does not fail over automatically because we prioritize data completeness and consistency over real-time availability for exports. HA data replication has inherent latency, which could result in incomplete or inconsistent exports during a failover.

### Failover Scenarios

**Namespace Failover with Healthy Primary Region**: When a namespace fails over to a secondary region but the primary region remains healthy (including its blob storage), the export job continues to operate from the primary region. It does not automatically switch to export data from the secondary region.

**Primary Region Outage**: If the primary region (where Export was configured) experiences a complete outage including S3/GCS storage: Exports will be unavailable until the primary region recovers. Once the primary region recovers, export will resume and include any workflow histories that occurred during the outage. There may be delays in export processing, but the complete dataset will eventually be available.
